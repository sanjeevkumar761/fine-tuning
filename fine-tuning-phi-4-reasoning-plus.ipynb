{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc26c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U transformers\n",
    "%pip install -U datasets \n",
    "%pip install -U accelerate \n",
    "%pip install -U peft \n",
    "%pip install -U trl \n",
    "%pip install -U bitsandbytes\n",
    "%pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1595a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ad0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"hf_tkn\"  # Use environment variables or replace with your token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461eafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load tokenizer & model\n",
    "model_dir = \"microsoft/Phi-4-reasoning-plus\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir, \n",
    "    device_map=\"auto\",  \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True             \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ca28cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ab5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_style=\"\"\"\n",
    "<|im_start|>system<|im_sep|>\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "<|im_end|>\n",
    "<|im_start|>user<|im_sep|>\n",
    "{}<|im_end|>\n",
    "<|im_start|>assistant<|im_sep|>\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d8e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Open-ended Verifiable Question\"]\n",
    "    complex_cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    texts = []\n",
    "    for input, cot, output in zip(inputs,complex_cots,outputs):\n",
    "        text = train_prompt_style.format(input,cot,output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af143e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"TheFinAI/Fino1_Reasoning_Path_FinQA\", split=\"train[0:1000]\", trust_remote_code=True\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")\n",
    "dataset[\"text\"][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb5161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884274c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_prompt_style = \"\"\"\n",
    "<|im_start|>system<|im_sep|>\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "<|im_end|>\n",
    "<|im_start|>user<|im_sep|>\n",
    "{}<|im_end|>\n",
    "<|im_start|>assistant<|im_sep|>\n",
    "<think>\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c338eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = dataset[20]['Open-ended Verifiable Question']\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question, \"\") + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"<|im_start|>assistant<|im_sep|>\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d2eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,                           # Scaling factor for LoRA\n",
    "    lora_dropout=0.05,                       # Add slight dropout for regularization\n",
    "    r=64,                                    # Rank of the LoRA update matrices\n",
    "    bias=\"none\",                             # No bias reparameterization\n",
    "    task_type=\"CAUSAL_LM\",                   # Task type: Causal Language Modeling\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # Target modules for LoRA\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46741a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "# Training Arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=0.2,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f5c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = dataset[20]['Open-ended Verifiable Question']\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question, \"\") + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"<|im_start|>assistant<|im_sep|>\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da23fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = dataset[200]['Open-ended Verifiable Question']\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question, \"\") + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"<|im_start|>assistant<|im_sep|>\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8503eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Get your username first\n",
    "api = HfApi()\n",
    "username = api.whoami()['name']\n",
    "model_name = \"Phi-4-Reasoning-Plus-FinQA-COT-2\"\n",
    "full_repo_name = f\"{username}/{model_name}\"\n",
    "\n",
    "print(f\"Creating repository: {full_repo_name}\")\n",
    "\n",
    "# Save the complete model (including LoRA adapters)\n",
    "trainer.save_model(f\"./{model_name}\")\n",
    "\n",
    "# Save tokenizer to the same directory\n",
    "tokenizer.save_pretrained(f\"./{model_name}\")\n",
    "\n",
    "# Check what files are saved locally\n",
    "print(\"Files in local model directory:\")\n",
    "for file in os.listdir(f\"./{model_name}\"):\n",
    "    print(f\"  {file}\")\n",
    "\n",
    "# Ensure the adapter files are present\n",
    "expected_files = [\"adapter_config.json\", \"adapter_model.safetensors\", \"tokenizer.json\", \"tokenizer_config.json\"]\n",
    "missing_files = []\n",
    "for file in expected_files:\n",
    "    if not os.path.exists(f\"./{model_name}/{file}\"):\n",
    "        missing_files.append(file)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"Missing files: {missing_files}\")\n",
    "    # Save the PEFT model explicitly\n",
    "    model.save_pretrained(f\"./{model_name}\")\n",
    "\n",
    "# Create repository with full name\n",
    "api.create_repo(repo_id=full_repo_name, exist_ok=True, private=False)\n",
    "\n",
    "# Upload the entire folder to HuggingFace Hub\n",
    "api.upload_folder(\n",
    "    folder_path=f\"./{model_name}\",\n",
    "    repo_id=full_repo_name,\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "print(f\"Model successfully uploaded to: https://huggingface.co/{full_repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a662e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load tokenizer & model\n",
    "model_dir = \"<your HF username>/Phi-4-Reasoning-Plus-FinQA-COT-2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir, \n",
    "    device_map=\"auto\",  \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True             \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2121934",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = dataset[200]['Open-ended Verifiable Question']\n",
    "print(question)\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question, \"\") + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"<|im_start|>assistant<|im_sep|>\")[1])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
